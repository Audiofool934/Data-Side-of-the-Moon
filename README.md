# Data Side of the Moon

## Decoding Pink Floyd’s Legacy

![Data Side of the Moon](media/DSOTM.png)

<p align = 'center'>
<i>artwork generated by DALL·E 3</i>
</p>

## Introduction

How can computer "precieve" music? How we, human, understand music? From a higher perspective, the process of precieve music is consist of 3 main steps: we hear(encode), we understand, and (if we want) we sing(decode) and then we hear (again). Base on this idea, we implement a neural network based on **convolutional autoencoder architecture**, and then explore the **learning representation of music**.

The key idea of this project is using **unsupervised learning** to explore music, which should be distinguished from <u>music genres classification tasks</u> which we consider has its own limitation due to the labels(genres) are limited by human ourselves. We believe that generes always falls behind music.

## Model "Echoes"

<p align="center">
<img src="media/architecture.png" alt="echoes_arc" width="500px">
"Echoes" model architecture
</p>


<p align="center">
<img src="media/train.png" alt="training" width="500px">
"Echoes" model training
</p>

<p align="center">
<img src="media/GTZAN_test.png" alt="GTZAN_test" width="500px">
Latent vectors of GTZAN testset
</p>

## Pink Floyd

<p align="center">
<img src="media/PF_album_year.png" alt="PF_album_year" width="500px">
PF albums - clusters - years
</p>

## Discussion and future work

- open world (classify things to unknown classes)

