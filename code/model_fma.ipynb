{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "from encoder import Encoder, load_encoder\n",
    "from decoder import Decoder, load_decoder\n",
    "from train import train_epoch, test_epoch\n",
    "# from utility import get_all_files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff6f62a0990>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN = False\n",
    "\n",
    "BETA = 1\n",
    "BATCH_SIZE = 64\n",
    "LATENT_SPACE_DIM = 1024\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "SAVE_ROUND = 20\n",
    "NUM_EPOCHS = 300\n",
    "\n",
    "torch.manual_seed(0) # random seed for reproducible results\n",
    "# 似乎不同设备要单独设置随机种子？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"data/spec/fma_small\"\n",
    "# dataset_path = \"data/spec/GTZAN_646\"\n",
    "model_save_path = \"models/Echoes\"\n",
    "csv_save_path = \"output/Echoes_output\"\n",
    "\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "os.makedirs(csv_save_path, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(f'device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:   5597\n",
      "Validation set size: 1198\n",
      "Test set size:       1198\n"
     ]
    }
   ],
   "source": [
    "class MusicDataset(Dataset):\n",
    "\tdef __init__(self, file_paths):\n",
    "\t\tself.file_paths = file_paths\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.file_paths)\n",
    "\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tfile_path = self.file_paths[idx]\n",
    "\t\tdata = np.load(file_path)\n",
    "\t\n",
    "\t\tif np.isnan(data).any():\n",
    "\t\t\tfilename = os.path.basename(file_path)\n",
    "\t\t\tlabel = filename.replace(\".npy\", \"\").lstrip(\"0\")\n",
    "\t\t\tprint(f\"Warning: NaN value found in {label}\")\n",
    "  \n",
    "\t\tdata = data[np.newaxis, :, :]  # Add a channel dimension\n",
    "\t\tdata = torch.tensor(data, dtype=torch.float32)\n",
    "\t\t\n",
    "\t\t# filename = os.path.basename(os.path.dirname(file_path))\n",
    "\t\tfilename = os.path.basename(file_path)\n",
    "\t\tlabel = filename.replace(\".npy\", \"\").lstrip(\"0\")\n",
    "\t\treturn data, label\n",
    "\n",
    "file_paths = []\n",
    "for root, dirs, files in os.walk(dataset_path):\n",
    "\tfor file in files:\n",
    "\t\tfile_paths.append(os.path.join(root, file))\n",
    "\n",
    "# 取出20%的数据作为测试集，10%的数据作为验证集，剩下的作为训练集（理论上每个文件夹有风格区别，应该尽量做到比例均匀），但是（先验的）风格不好分，暂时直接随机划分\n",
    "\n",
    "m = len(file_paths)\n",
    "test_size = int(m * 0.15)\n",
    "valid_size = int(m * 0.15)\n",
    "train_size = m - test_size - valid_size\n",
    "# train : valid : test = 70 : 15 : 15\n",
    "\n",
    "paths = np.array(file_paths)\n",
    "np.random.shuffle(paths)\n",
    "\n",
    "train_paths = paths[:train_size]\n",
    "valid_paths = paths[train_size:train_size + valid_size]\n",
    "test_paths = paths[train_size + valid_size:]\n",
    "\n",
    "train_dataset = MusicDataset(train_paths)\n",
    "valid_dataset = MusicDataset(valid_paths)\n",
    "test_dataset = MusicDataset(test_paths)\n",
    "\n",
    "print(f'Training set size:   {len(train_dataset)}')\n",
    "print(f'Validation set size: {len(valid_dataset)}')\n",
    "print(f'Test set size:       {len(test_dataset)}')\n",
    "\n",
    "if TRAIN:\n",
    "\ttrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\tvalid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\ttest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "\tloss_fn = torch.nn.MSELoss()\n",
    "\t\n",
    "\tencoder = Encoder(encoded_space_dim=LATENT_SPACE_DIM)\n",
    "\tdecoder = Decoder(encoded_space_dim=LATENT_SPACE_DIM)\n",
    "\tparams_to_optimize = [\n",
    "\t\t{'params': encoder.parameters()},\n",
    "\t\t{'params': decoder.parameters()}\n",
    "\t]\n",
    "\n",
    "\toptim = torch.optim.Adam(params_to_optimize, lr=LEARNING_RATE, weight_decay=1e-05)\n",
    "\n",
    "\tencoder = encoder.to(device)\n",
    "\tdecoder = decoder.to(device)\n",
    "\t\n",
    "\tlosses = {'train_loss':[],'val_loss':[]}\n",
    "\n",
    "\tfor epoch in range(NUM_EPOCHS):\n",
    "\t\t# beta = epoch / NUM_EPOCHS  # Increase beta over time\n",
    "\t\tbeta = BETA\n",
    "\t\ttrain_loss =train_epoch(encoder, decoder, device, train_loader, loss_fn, optim, beta)\n",
    "\t\tval_loss = test_epoch(encoder, decoder, device, valid_loader, loss_fn, beta)\n",
    "\t\n",
    "\t\tprint('\\n EPOCH {}/{} \\t train loss {} \\t val loss {}'.format(epoch + 1, NUM_EPOCHS,train_loss,val_loss))\n",
    "\n",
    "\t\t# track losses\n",
    "\t\tlosses['train_loss'].append(train_loss)\n",
    "\t\tlosses['val_loss'].append(val_loss)\n",
    "\n",
    "\t\t# save model\n",
    "\t\tif (epoch + 1) % SAVE_ROUND == 0: \n",
    "\t\t\ttorch.save(encoder.state_dict(), f'{model_save_path}/encoder_{epoch+1}.pth')\n",
    "\t\t\ttorch.save(decoder.state_dict(), f'{model_save_path}/decoder_{epoch+1}.pth')\n",
    "\n",
    "\ttest_loss = test_epoch(encoder, decoder, device, test_loader, loss_fn, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogy(\u001b[43mlosses\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39msemilogy(losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 800x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.semilogy(losses['train_loss'], label='Train')\n",
    "plt.semilogy(losses['val_loss'], label='Valid')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Average Loss|')\n",
    "# plt.grid()\n",
    "plt.legend()\n",
    "plt.title('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_loss\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loss' is not defined"
     ]
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m encoder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/encoder.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m decoder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/decoder.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mencoder\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), encoder_path)\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(decoder\u001b[38;5;241m.\u001b[39mstate_dict(), decoder_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
     ]
    }
   ],
   "source": [
    "encoder_path = f\"{model_save_path}/encoder.pth\"\n",
    "decoder_path = f\"{model_save_path}/decoder.pth\"\n",
    "torch.save(encoder.state_dict(), encoder_path)\n",
    "torch.save(decoder.state_dict(), decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chuangyan/Data-Side-of-the-Moon/code/encoder.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")))\n",
      "100%|██████████| 1198/1198 [00:03<00:00, 306.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>enc. v 0</th>\n",
       "      <th>enc. v 1</th>\n",
       "      <th>enc. v 2</th>\n",
       "      <th>enc. v 3</th>\n",
       "      <th>enc. v 4</th>\n",
       "      <th>enc. v 5</th>\n",
       "      <th>enc. v 6</th>\n",
       "      <th>enc. v 7</th>\n",
       "      <th>enc. v 8</th>\n",
       "      <th>enc. v 9</th>\n",
       "      <th>...</th>\n",
       "      <th>enc. v 1015</th>\n",
       "      <th>enc. v 1016</th>\n",
       "      <th>enc. v 1017</th>\n",
       "      <th>enc. v 1018</th>\n",
       "      <th>enc. v 1019</th>\n",
       "      <th>enc. v 1020</th>\n",
       "      <th>enc. v 1021</th>\n",
       "      <th>enc. v 1022</th>\n",
       "      <th>enc. v 1023</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.946606</td>\n",
       "      <td>-0.425239</td>\n",
       "      <td>-2.659478</td>\n",
       "      <td>0.162394</td>\n",
       "      <td>-0.106082</td>\n",
       "      <td>-0.586880</td>\n",
       "      <td>-0.604333</td>\n",
       "      <td>-0.326373</td>\n",
       "      <td>-1.081557</td>\n",
       "      <td>-0.359277</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.900775</td>\n",
       "      <td>-1.080082</td>\n",
       "      <td>0.316355</td>\n",
       "      <td>0.337315</td>\n",
       "      <td>-0.339624</td>\n",
       "      <td>0.955027</td>\n",
       "      <td>-1.255026</td>\n",
       "      <td>-1.195891</td>\n",
       "      <td>-0.105890</td>\n",
       "      <td>74388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.153706</td>\n",
       "      <td>-0.557118</td>\n",
       "      <td>0.899824</td>\n",
       "      <td>-0.714434</td>\n",
       "      <td>1.396821</td>\n",
       "      <td>0.869281</td>\n",
       "      <td>0.141491</td>\n",
       "      <td>-0.937902</td>\n",
       "      <td>0.799757</td>\n",
       "      <td>0.872251</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.722580</td>\n",
       "      <td>-1.036961</td>\n",
       "      <td>-0.508824</td>\n",
       "      <td>0.724544</td>\n",
       "      <td>0.100975</td>\n",
       "      <td>-1.723591</td>\n",
       "      <td>-0.841320</td>\n",
       "      <td>-0.307872</td>\n",
       "      <td>1.502998</td>\n",
       "      <td>70774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.511299</td>\n",
       "      <td>-0.717655</td>\n",
       "      <td>-0.519853</td>\n",
       "      <td>0.148019</td>\n",
       "      <td>0.111525</td>\n",
       "      <td>-0.411314</td>\n",
       "      <td>0.907526</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>-0.700670</td>\n",
       "      <td>0.134560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.833435</td>\n",
       "      <td>0.387396</td>\n",
       "      <td>1.405884</td>\n",
       "      <td>0.841048</td>\n",
       "      <td>0.972707</td>\n",
       "      <td>0.032631</td>\n",
       "      <td>-0.392905</td>\n",
       "      <td>1.489119</td>\n",
       "      <td>-1.306501</td>\n",
       "      <td>67016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.325960</td>\n",
       "      <td>-1.427279</td>\n",
       "      <td>-0.861544</td>\n",
       "      <td>0.333140</td>\n",
       "      <td>-0.803067</td>\n",
       "      <td>0.727766</td>\n",
       "      <td>-0.424727</td>\n",
       "      <td>0.413645</td>\n",
       "      <td>-0.140047</td>\n",
       "      <td>-0.247262</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.215967</td>\n",
       "      <td>-0.910874</td>\n",
       "      <td>0.425684</td>\n",
       "      <td>-0.322223</td>\n",
       "      <td>1.161427</td>\n",
       "      <td>-0.297772</td>\n",
       "      <td>-0.081139</td>\n",
       "      <td>0.234709</td>\n",
       "      <td>0.092129</td>\n",
       "      <td>140259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.370955</td>\n",
       "      <td>0.620888</td>\n",
       "      <td>0.545438</td>\n",
       "      <td>-0.274665</td>\n",
       "      <td>0.054474</td>\n",
       "      <td>-1.041744</td>\n",
       "      <td>0.287337</td>\n",
       "      <td>0.654044</td>\n",
       "      <td>-0.173162</td>\n",
       "      <td>1.448630</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.157629</td>\n",
       "      <td>1.303216</td>\n",
       "      <td>-0.125188</td>\n",
       "      <td>-0.714768</td>\n",
       "      <td>1.484253</td>\n",
       "      <td>3.224166</td>\n",
       "      <td>-0.414848</td>\n",
       "      <td>0.841281</td>\n",
       "      <td>0.273222</td>\n",
       "      <td>97393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1193</th>\n",
       "      <td>1.618409</td>\n",
       "      <td>-2.107481</td>\n",
       "      <td>-0.294493</td>\n",
       "      <td>-0.587241</td>\n",
       "      <td>-0.084038</td>\n",
       "      <td>0.207361</td>\n",
       "      <td>-0.595117</td>\n",
       "      <td>1.163033</td>\n",
       "      <td>-0.929993</td>\n",
       "      <td>-0.155797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038489</td>\n",
       "      <td>-1.872512</td>\n",
       "      <td>-1.001752</td>\n",
       "      <td>-1.695376</td>\n",
       "      <td>-2.360294</td>\n",
       "      <td>0.044683</td>\n",
       "      <td>-1.196657</td>\n",
       "      <td>1.012713</td>\n",
       "      <td>-2.824301</td>\n",
       "      <td>97211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1194</th>\n",
       "      <td>1.075054</td>\n",
       "      <td>-0.508127</td>\n",
       "      <td>1.437324</td>\n",
       "      <td>1.397717</td>\n",
       "      <td>-2.181241</td>\n",
       "      <td>-0.535416</td>\n",
       "      <td>-1.249104</td>\n",
       "      <td>-0.114818</td>\n",
       "      <td>0.111373</td>\n",
       "      <td>1.085199</td>\n",
       "      <td>...</td>\n",
       "      <td>1.044502</td>\n",
       "      <td>0.012442</td>\n",
       "      <td>-0.293274</td>\n",
       "      <td>-0.253205</td>\n",
       "      <td>0.347142</td>\n",
       "      <td>-0.169346</td>\n",
       "      <td>-0.412717</td>\n",
       "      <td>0.034062</td>\n",
       "      <td>-1.604117</td>\n",
       "      <td>107912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1195</th>\n",
       "      <td>-0.119992</td>\n",
       "      <td>-0.400481</td>\n",
       "      <td>0.973900</td>\n",
       "      <td>0.299499</td>\n",
       "      <td>-0.928898</td>\n",
       "      <td>-0.505174</td>\n",
       "      <td>1.579869</td>\n",
       "      <td>0.656737</td>\n",
       "      <td>-0.811004</td>\n",
       "      <td>-0.174451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.910894</td>\n",
       "      <td>-0.055492</td>\n",
       "      <td>1.273955</td>\n",
       "      <td>1.067598</td>\n",
       "      <td>0.959377</td>\n",
       "      <td>0.365274</td>\n",
       "      <td>0.324394</td>\n",
       "      <td>-1.322098</td>\n",
       "      <td>-0.652902</td>\n",
       "      <td>114223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>-0.108738</td>\n",
       "      <td>1.671015</td>\n",
       "      <td>-0.976587</td>\n",
       "      <td>-0.006499</td>\n",
       "      <td>0.702491</td>\n",
       "      <td>0.928739</td>\n",
       "      <td>-0.242716</td>\n",
       "      <td>-0.183750</td>\n",
       "      <td>0.360491</td>\n",
       "      <td>0.523292</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.482288</td>\n",
       "      <td>-0.040770</td>\n",
       "      <td>-1.196344</td>\n",
       "      <td>0.630051</td>\n",
       "      <td>-1.146848</td>\n",
       "      <td>0.650762</td>\n",
       "      <td>0.103186</td>\n",
       "      <td>-0.699240</td>\n",
       "      <td>0.403714</td>\n",
       "      <td>72477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1197</th>\n",
       "      <td>1.288307</td>\n",
       "      <td>0.933214</td>\n",
       "      <td>-0.318268</td>\n",
       "      <td>-0.219178</td>\n",
       "      <td>-1.555906</td>\n",
       "      <td>0.917101</td>\n",
       "      <td>-0.546207</td>\n",
       "      <td>0.658253</td>\n",
       "      <td>-0.847216</td>\n",
       "      <td>0.713137</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.494468</td>\n",
       "      <td>-0.109268</td>\n",
       "      <td>-0.849779</td>\n",
       "      <td>-0.348644</td>\n",
       "      <td>1.353736</td>\n",
       "      <td>0.220385</td>\n",
       "      <td>1.363137</td>\n",
       "      <td>1.889768</td>\n",
       "      <td>-1.286689</td>\n",
       "      <td>6330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1198 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      enc. v 0  enc. v 1  enc. v 2  enc. v 3  enc. v 4  enc. v 5  enc. v 6  \\\n",
       "0    -0.946606 -0.425239 -2.659478  0.162394 -0.106082 -0.586880 -0.604333   \n",
       "1     0.153706 -0.557118  0.899824 -0.714434  1.396821  0.869281  0.141491   \n",
       "2     2.511299 -0.717655 -0.519853  0.148019  0.111525 -0.411314  0.907526   \n",
       "3     0.325960 -1.427279 -0.861544  0.333140 -0.803067  0.727766 -0.424727   \n",
       "4    -0.370955  0.620888  0.545438 -0.274665  0.054474 -1.041744  0.287337   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1193  1.618409 -2.107481 -0.294493 -0.587241 -0.084038  0.207361 -0.595117   \n",
       "1194  1.075054 -0.508127  1.437324  1.397717 -2.181241 -0.535416 -1.249104   \n",
       "1195 -0.119992 -0.400481  0.973900  0.299499 -0.928898 -0.505174  1.579869   \n",
       "1196 -0.108738  1.671015 -0.976587 -0.006499  0.702491  0.928739 -0.242716   \n",
       "1197  1.288307  0.933214 -0.318268 -0.219178 -1.555906  0.917101 -0.546207   \n",
       "\n",
       "      enc. v 7  enc. v 8  enc. v 9  ...  enc. v 1015  enc. v 1016  \\\n",
       "0    -0.326373 -1.081557 -0.359277  ...    -1.900775    -1.080082   \n",
       "1    -0.937902  0.799757  0.872251  ...    -0.722580    -1.036961   \n",
       "2     0.808997 -0.700670  0.134560  ...    -0.833435     0.387396   \n",
       "3     0.413645 -0.140047 -0.247262  ...    -0.215967    -0.910874   \n",
       "4     0.654044 -0.173162  1.448630  ...    -0.157629     1.303216   \n",
       "...        ...       ...       ...  ...          ...          ...   \n",
       "1193  1.163033 -0.929993 -0.155797  ...     0.038489    -1.872512   \n",
       "1194 -0.114818  0.111373  1.085199  ...     1.044502     0.012442   \n",
       "1195  0.656737 -0.811004 -0.174451  ...    -0.910894    -0.055492   \n",
       "1196 -0.183750  0.360491  0.523292  ...    -0.482288    -0.040770   \n",
       "1197  0.658253 -0.847216  0.713137  ...    -0.494468    -0.109268   \n",
       "\n",
       "      enc. v 1017  enc. v 1018  enc. v 1019  enc. v 1020  enc. v 1021  \\\n",
       "0        0.316355     0.337315    -0.339624     0.955027    -1.255026   \n",
       "1       -0.508824     0.724544     0.100975    -1.723591    -0.841320   \n",
       "2        1.405884     0.841048     0.972707     0.032631    -0.392905   \n",
       "3        0.425684    -0.322223     1.161427    -0.297772    -0.081139   \n",
       "4       -0.125188    -0.714768     1.484253     3.224166    -0.414848   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1193    -1.001752    -1.695376    -2.360294     0.044683    -1.196657   \n",
       "1194    -0.293274    -0.253205     0.347142    -0.169346    -0.412717   \n",
       "1195     1.273955     1.067598     0.959377     0.365274     0.324394   \n",
       "1196    -1.196344     0.630051    -1.146848     0.650762     0.103186   \n",
       "1197    -0.849779    -0.348644     1.353736     0.220385     1.363137   \n",
       "\n",
       "      enc. v 1022  enc. v 1023   label  \n",
       "0       -1.195891    -0.105890   74388  \n",
       "1       -0.307872     1.502998   70774  \n",
       "2        1.489119    -1.306501   67016  \n",
       "3        0.234709     0.092129  140259  \n",
       "4        0.841281     0.273222   97393  \n",
       "...           ...          ...     ...  \n",
       "1193     1.012713    -2.824301   97211  \n",
       "1194     0.034062    -1.604117  107912  \n",
       "1195    -1.322098    -0.652902  114223  \n",
       "1196    -0.699240     0.403714   72477  \n",
       "1197     1.889768    -1.286689    6330  \n",
       "\n",
       "[1198 rows x 1025 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not TRAIN:\n",
    "\tencoder_path = f\"{model_save_path}/encoder_280.pth\"\n",
    "\tencoder = load_encoder(encoder_path, LATENT_SPACE_DIM)\n",
    "\tencoder = encoder.to(device)\n",
    "\n",
    "encoded_samples = []\n",
    "for sample in tqdm(test_dataset):\n",
    "\tdata = sample[0].unsqueeze(0).to(device)\n",
    "\tlabel = sample[1]\n",
    "\tencoder.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tmu, log_var = encoder(data)\n",
    "\n",
    "\tencoded_data = mu + torch.exp(0.5 * log_var) * torch.randn_like(mu)\n",
    "\tencoded_data = encoded_data.flatten().cpu().numpy()\n",
    "\tencoded_sample = {f\"enc. v {i}\": enc for i, enc in enumerate(encoded_data)}\n",
    "\tencoded_sample['label'] = label\n",
    "\tencoded_samples.append(encoded_sample)\n",
    "\n",
    "encoded_samples = pd.DataFrame(encoded_samples)\n",
    "encoded_samples.to_csv(f\"{csv_save_path}/fma_small_encoded.csv\", index=False)\n",
    "encoded_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 128, 323]             320\n",
      "       BatchNorm2d-2         [-1, 32, 128, 323]              64\n",
      "         LeakyReLU-3         [-1, 32, 128, 323]               0\n",
      "            Conv2d-4          [-1, 64, 64, 162]          18,496\n",
      "       BatchNorm2d-5          [-1, 64, 64, 162]             128\n",
      "         LeakyReLU-6          [-1, 64, 64, 162]               0\n",
      "            Conv2d-7          [-1, 128, 32, 81]          73,856\n",
      "       BatchNorm2d-8          [-1, 128, 32, 81]             256\n",
      "         LeakyReLU-9          [-1, 128, 32, 81]               0\n",
      "           Conv2d-10          [-1, 256, 16, 41]         295,168\n",
      "      BatchNorm2d-11          [-1, 256, 16, 41]             512\n",
      "        LeakyReLU-12          [-1, 256, 16, 41]               0\n",
      "          Flatten-13               [-1, 167936]               0\n",
      "           Linear-14                 [-1, 1024]     171,967,488\n",
      "           Linear-15                 [-1, 1024]     171,967,488\n",
      "================================================================\n",
      "Total params: 344,323,776\n",
      "Trainable params: 344,323,776\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.63\n",
      "Forward/backward pass size (MB): 58.20\n",
      "Params size (MB): 1313.49\n",
      "Estimated Total Size (MB): 1372.32\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(encoder, input_size=(1, 256, 646))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
